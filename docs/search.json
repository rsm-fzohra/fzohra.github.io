[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "FatmaZ Resume",
    "section": "",
    "text": "Download PDF file.\nAbout this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FatmaZ website",
    "section": "",
    "text": "Welcome to my website\nI am a quantitative marketing research scientist :)\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Customer Distribution by Region\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nFatma Zohra\n\n\nMay 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nFatma Zohra\n\n\nMay 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Examples\n\n\n\n\n\n\nFatma Zohra\n\n\nMay 21, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/Project 3/hw3_questions.html",
    "href": "projects/Project 3/hw3_questions.html",
    "title": "Multinomial Logit Examples",
    "section": "",
    "text": "This assignment uses uses the MNL model to analyze (1) yogurt purchase data made by consumers at a retail location, and (2) conjoint data about consumer preferences for minivans."
  },
  {
    "objectID": "projects/Project 3/hw3_questions.html#estimating-yogurt-preferences",
    "href": "projects/Project 3/hw3_questions.html#estimating-yogurt-preferences",
    "title": "Multinomial Logit Examples",
    "section": "1. Estimating Yogurt Preferences",
    "text": "1. Estimating Yogurt Preferences\n\nLikelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 4 products, then either \\(y=3\\) or \\(y=(0,0,1,0)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, size, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 4 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta} + e^{x_4'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=\\delta_{i4}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 \\times \\mathbb{P}_i(4)^0 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]\n\n\nYogurt Dataset\n\nimport pandas as pd\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\n\nyogurt = pd.read_csv('/home/jovyan/Desktop/MGTA495-2/projects/Project 3/yogurt_data.csv')\nyogurt.head()\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0.108\n0.081\n0.061\n0.079\n\n\n1\n2\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.064\n0.075\n\n\n2\n3\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n3\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0.108\n0.098\n0.061\n0.086\n\n\n4\n5\n0\n1\n0\n0\n0\n0\n0\n0\n0.125\n0.098\n0.049\n0.079\n\n\n\n\n\n\n\n\nyogurt.shape\n\n(2430, 13)\n\n\n\nyogurt[['y1','y2','y3','y4']].sum(axis = 0)\n\ny1    831\ny2    975\ny3     71\ny4    553\ndtype: int64\n\n\n\nyogurt.describe()\n\n\n\n\n\n\n\n\nid\ny1\ny2\ny3\ny4\nf1\nf2\nf3\nf4\np1\np2\np3\np4\n\n\n\n\ncount\n2430.0000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n2430.000000\n\n\nmean\n1215.5000\n0.341975\n0.401235\n0.029218\n0.227572\n0.055556\n0.039506\n0.037449\n0.037449\n0.106248\n0.081532\n0.053622\n0.079507\n\n\nstd\n701.6249\n0.474469\n0.490249\n0.168452\n0.419351\n0.229109\n0.194836\n0.189897\n0.189897\n0.020587\n0.011047\n0.008054\n0.007714\n\n\nmin\n1.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n-0.012000\n0.000000\n0.025000\n0.004000\n\n\n25%\n608.2500\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.103000\n0.081000\n0.050000\n0.079000\n\n\n50%\n1215.5000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.108000\n0.086000\n0.054000\n0.079000\n\n\n75%\n1822.7500\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.115000\n0.086000\n0.061000\n0.086000\n\n\nmax\n2430.0000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n0.193000\n0.111000\n0.086000\n0.104000"
  },
  {
    "objectID": "projects/Project 3/hw3_questions.html#estimating-minivan-preferences",
    "href": "projects/Project 3/hw3_questions.html#estimating-minivan-preferences",
    "title": "Multinomial Logit Examples",
    "section": "2. Estimating Minivan Preferences",
    "text": "2. Estimating Minivan Preferences\n\nData\nBasic Information about the data: The survey contains 9000 rows, with 9 columns. Price ranges from 30,000 to 40,000 with a mean of 35,003.89. Seats are in the configuration of 6,7 and 8 with a mean of 7. About 33.33% of the alternatives have been chosen, indicating that the data includes one choice per three alternatives. Uniqueness in Data: - Number of respondents: 200 - Number of choice tasks: 15 by each respondent - Number of alternatives: 3 alternatives per task\n\ndata = pd.read_csv(\"/home/jovyan/Desktop/MGTA495-2/projects/Project 3/rintro-chapter13conjoint.csv\")\n\n\ndata.head()\n\n\n\n\n\n\n\n\nresp.id\nques\nalt\ncarpool\nseat\ncargo\neng\nprice\nchoice\n\n\n\n\n0\n1\n1\n1\nyes\n6\n2ft\ngas\n35\n0\n\n\n1\n1\n1\n2\nyes\n8\n3ft\nhyb\n30\n0\n\n\n2\n1\n1\n3\nyes\n6\n3ft\ngas\n30\n1\n\n\n3\n1\n2\n1\nyes\n6\n2ft\ngas\n30\n0\n\n\n4\n1\n2\n2\nyes\n7\n3ft\ngas\n35\n1\n\n\n\n\n\n\n\n\ndata['resp.id'].nunique()\n\n200\n\n\n\ndata['ques'].nunique()\n\n15\n\n\n\ndata['alt'].nunique()\n\n3\n\n\n\ndata.shape\n\n(9000, 9)\n\n\nThe attributes (levels) were number of seats (6,7,8), cargo space (2ft, 3ft), engine type (gas, hybrid, electric), and price (in thousands of dollars).\n\n\nModel\n\nfrom patsy import dmatrices\nimport statsmodels.api as sm\n\nformula = 'choice ~ C(seat, Treatment(6)) + C(cargo, Treatment(\"2ft\")) + C(eng, Treatment(\"gas\")) + price'\n\ny, X = dmatrices(formula, data, return_type='dataframe')\n\n# Fit the MNL model\nmnl_model = sm.MNLogit(y, X)\nmnl_result = mnl_model.fit()\n\nmnl_result.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.558663\n         Iterations 6\n\n\n\nMNLogit Regression Results\n\n\nDep. Variable:\nchoice\nNo. Observations:\n9000\n\n\nModel:\nMNLogit\nDf Residuals:\n8993\n\n\nMethod:\nMLE\nDf Model:\n6\n\n\nDate:\nTue, 21 May 2024\nPseudo R-squ.:\n0.1223\n\n\nTime:\n19:31:28\nLog-Likelihood:\n-5028.0\n\n\nconverged:\nTrue\nLL-Null:\n-5728.6\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n1.252e-299\n\n\n\n\n\n\nchoice=1\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n5.5322\n0.224\n24.677\n0.000\n5.093\n5.972\n\n\nC(seat, Treatment(6))[T.7]\n-0.5248\n0.060\n-8.800\n0.000\n-0.642\n-0.408\n\n\nC(seat, Treatment(6))[T.8]\n-0.2931\n0.059\n-5.009\n0.000\n-0.408\n-0.178\n\n\nC(cargo, Treatment(\"2ft\"))[T.3ft]\n0.4385\n0.049\n9.004\n0.000\n0.343\n0.534\n\n\nC(eng, Treatment(\"gas\"))[T.elec]\n-1.4347\n0.062\n-23.217\n0.000\n-1.556\n-1.314\n\n\nC(eng, Treatment(\"gas\"))[T.hyb]\n-0.7605\n0.057\n-13.361\n0.000\n-0.872\n-0.649\n\n\nprice\n-0.1591\n0.006\n-25.616\n0.000\n-0.171\n-0.147\n\n\n\n\n\n\n\nResults\nCoefficients Interpretation Intercept: The large positive intercept (5.5322) suggests a strong baseline propensity towards choosing an alternative when all variables are at their reference levels (6 seats, 2ft cargo, gas engine, and a base price).\nSeats:\n7 seats (C(seat, Treatment(6))[T.7]): The negative coefficient (-0.5248) indicates that, holding other factors constant, consumers are less likely to choose minivans with 7 seats compared to 6 seats. 8 seats (C(seat, Treatment(6))[T.8]): Similarly, the negative coefficient (-0.2931) suggests that minivans with 8 seats are also less preferred than those with 6 seats, though the effect is smaller than for 7 seats.\nCargo: 3ft (C(cargo, Treatment(“2ft”))[T.3ft]): The positive coefficient (0.4385) indicates a preference for 3ft of cargo space over 2ft, suggesting that consumers value larger cargo space.\nEngine Type: Electric (C(eng, Treatment(“gas”))[T.elec]): The negative coefficient (-1.4347) strongly indicates that electric engines are less preferred compared to gas engines. Hybrid (C(eng, Treatment(“gas”))[T.hyb]): The negative coefficient (-0.7605) shows that hybrids are also less preferred than gas engines, but the disfavor is less pronounced compared to electric engines.\nPrice: The negative coefficient (-0.1591) for price confirms the expected behavior: as price increases, the likelihood of choosing that minivan decreases.\nStatistical Significance All predictors are statistically significant (p-values &lt; 0.000), indicating strong evidence against the null hypothesis for each coefficient.\nStandard Errors The standard errors provide an estimate of the standard deviation of the coefficients’ sampling distribution. Smaller values indicate more precise estimates. For instance, the relatively low standard errors for the engine type coefficients (0.062 and 0.057) suggest that these estimates are precise.\nModel Fit and Metrics Pseudo R-squared (0.1223): This value suggests that the model explains about 12.23% of the variance in choice relative to a model without predictors. This isn’t particularly high, but it’s not unusual in choice modeling where many unobserved factors can influence decisions. Log-Likelihood and LL-Null: The difference in log-likelihood between the model and the null model (LL-Null) indicates that the predictors significantly improve the fit of the model.\nSummary The model suggests that consumers prefer: 6 seats over 7 or 8 seats. More cargo space (3ft over 2ft). Gas engines over electric or hybrid engines. Lower prices.\nConsumers are willing to pay $2750 for an foot of space from 2 ft cargo to 3 ft cargo\n\n# Extract the price coefficient\nprice_coef = mnl_result.params.loc['price'][0]\n\n# Extract the coefficient for cargo space (3ft vs 2ft)\ncargo_coef = mnl_result.params.loc['C(cargo, Treatment(\"2ft\"))[T.3ft]'][0]\n\n# Calculate the dollar value of 3ft of cargo space compared to 2ft\ndollar_value_cargo = cargo_coef / abs(price_coef)\ndollar_value_cargo\n\n2.755802315180663\n\n\ntodo: assume the market consists of the following 6 minivans. Predict the market shares of each minivan in the market.\n\n\n\nMinivan\nSeats\nCargo\nEngine\nPrice\n\n\n\n\nA\n7\n2\nHyb\n30\n\n\nB\n6\n2\nGas\n30\n\n\nC\n8\n2\nGas\n30\n\n\nD\n7\n3\nGas\n40\n\n\nE\n6\n2\nElec\n40\n\n\nF\n7\n2\nHyb\n35\n\n\n\n\ncoefficients = {\n    'Intercept': 5.5322,\n    'C(seat, Treatment(6))[T.7]': -0.5248,\n    'C(seat, Treatment(6))[T.8]': -0.2931,\n    'C(cargo, Treatment(\"2ft\"))[T.3ft]': 0.4385,\n    'C(eng, Treatment(\"gas\"))[T.elec]': -1.4347,\n    'C(eng, Treatment(\"gas\"))[T.hyb]': -0.7605,\n    'price': -0.1591\n}\n\nnew_data = pd.DataFrame({\n    \"seat\": [7, 6, 8, 7, 6, 7],\n    \"cargo\": ['2ft', '2ft', '2ft', '3ft', '2ft', '2ft'],\n    \"eng\": ['Hyb', 'Gas', 'Gas', 'Gas', 'Elec', 'Hyb'],\n    \"price\": [30, 30, 30, 40, 40, 35]\n})\n\n# Calculate utility for each minivan\ndef calculate_utility(row):\n    utility = coefficients['Intercept']\n    utility += coefficients['C(seat, Treatment(6))[T.7]'] * (row['seat'] == 7)\n    utility += coefficients['C(seat, Treatment(6))[T.8]'] * (row['seat'] == 8)\n    utility += coefficients['C(cargo, Treatment(\"2ft\"))[T.3ft]'] * (row['cargo'] == '3ft')\n    utility += coefficients['C(eng, Treatment(\"gas\"))[T.elec]'] * (row['eng'] == 'Elec')\n    utility += coefficients['C(eng, Treatment(\"gas\"))[T.hyb]'] * (row['eng'] == 'Hyb')\n    utility += coefficients['price'] * row['price']\n    return utility\n\nnew_data['utility'] = new_data.apply(calculate_utility, axis=1)\n\n# Compute probabilities using softmax\ndef softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x / e_x.sum()\n\nnew_data['probability'] = softmax(new_data['utility'])\n\nprint(new_data[['probability']])\n\n   probability\n0     0.116071\n1     0.419684\n2     0.313062\n3     0.078430\n4     0.020365\n5     0.052389\n\n\nMarket Share for each of the sox minivans\nMinivan A: 11.61% Minivan B: 41.97% Minivan C: 31.31% Minivan D: 7.84% Minivan E: 2.04% Minivan F: 5.24%\nInterpretation of Probabilities\nMinivan A (0.116071): This model, with 7 seats, 2ft of cargo space, a hybrid engine, and a price of $30,000, has a predicted market share of about 11.6%. This suggests it’s a moderately appealing option, likely benefiting from its hybrid engine and lower price.\nMinivan B (0.419684): This minivan, which features 6 seats, 2ft of cargo space, a gasoline engine, and a price of $30,000, has the highest predicted market share at approximately 41.97%. Its appeal is likely due to having the baseline level for seats (6) and engine type (gas), combined with a lower price, aligning well with consumer preferences.\nMinivan C (0.313062): With 8 seats, 2ft of cargo, a gasoline engine, and also priced at $30,000, this model has a substantial market share of about 31.31%. Its higher number of seats might appeal to those needing more seating capacity, despite the generally lower preference for more than 6 seats.\nMinivan D (0.078430): This minivan offers 7 seats, 3ft of cargo space, a gasoline engine, and is priced at $40,000, which results in a lower predicted market share of approximately 7.84%. The higher price and possibly less preferred seating configuration contribute to its lower attractiveness.\nMinivan E (0.020365): Featuring 6 seats, 2ft of cargo, an electric engine, and a higher price of $40,000, this model has the lowest market share at about 2.04%. The electric engine and higher price significantly reduce its appeal, as reflected in the model coefficients.\nMinivan F (0.052389): This minivan, similar to A but priced at $35,000, sees a reduced market share of about 5.24%. The increase in price compared to Minivan A likely accounts for its reduced attractiveness despite similar other features.\n\n\nOverall Insights\nPrice Sensitivity: There’s a clear sensitivity to price changes, with cheaper models generally having higher market shares. Preference for Gas Engines: Models with gas engines are more preferred over hybrid or electric, especially at the same price points. Impact of Seat Configuration: While baseline seat configurations (6 seats) tend to be more popular, there’s notable interest in models with more seats (8), provided the price remains competitive. Cargo Space: The effect of cargo space isn’t clearly dominant in these predictions, as only Minivan D offered more cargo space but at a higher price, affecting its overall appeal."
  },
  {
    "objectID": "projects/Project 3/hw3_questions.html#dataset-overview",
    "href": "projects/Project 3/hw3_questions.html#dataset-overview",
    "title": "Multinomial Logit Examples",
    "section": "Dataset Overview:",
    "text": "Dataset Overview:\nThe dataset consists of several columns that capture various aspects of the yogurt purchasing decision. 1- ID: A unique identifier for each customer 2- y1,y2,y3,y4: Binary indicators representing whether a customer preferred a particulat yogurt brand, 1 being preferred and 0 being not preferred 3- f1,f2,f3,f4: Binary indicators showing whether each yogurt was featrued in a promotional display 4- p1,p2,p3,p4: Prices of the respective yogurt brabds at the time of purchase"
  },
  {
    "objectID": "projects/Project 3/hw3_questions.html#analysis-goals",
    "href": "projects/Project 3/hw3_questions.html#analysis-goals",
    "title": "Multinomial Logit Examples",
    "section": "Analysis Goals",
    "text": "Analysis Goals\nUsing the MNL Model, we aim to achieve the following: 1- Estimate Customer Preferences: Determine which yogurt brand is most preferred based on estimated coefficients. 2- Price Senstivity Analysis: Understand how changes in prices influence consumer choices. 3- Market Share Simulation: Simulate market share changes under different pricing scenarios.\nLet the vector of product features include brand dummy variables for yogurts 1-3 (we’ll omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate if a yogurt was featured, and a continuous variable for the yogurts’ prices:\n\\[ x_j' = [\\mathbbm{1}(\\text{Yogurt 1}), \\mathbbm{1}(\\text{Yogurt 2}), \\mathbbm{1}(\\text{Yogurt 3}), X_f, X_p] \\]\n\\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)).\nWhat we would like to do is reorganize the data from a “wide” shape with \\(n\\) rows and multiple columns for each covariate, to a “long” shape with \\(n \\times J\\) rows and a single column for each covariate. As part of this re-organization, we’ll add binary variables to indicate the first 3 products; the variables for featured and price are included in the dataset and simply need to be “pivoted” or “melted” from wide to long.\nReshape and prep the data: The “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate)\n\nyogurt_long = pd.wide_to_long(yogurt, stubnames=['y','f', 'p'], i=['id'], j='product').reset_index()\n\n# Add product brand dummies\nyogurt_long['yogurt1'] = (yogurt_long['product'] == 1).astype(int)\nyogurt_long['yogurt2'] = (yogurt_long['product'] == 2).astype(int)\nyogurt_long['yogurt3'] = (yogurt_long['product'] == 3).astype(int)\n\n\n# Rename columns and drop unnecessary ones\nyogurt_long.rename(columns={'f': 'featured', 'p': 'price'}, inplace=True)\n\n# Display the DataFrame\nyogurt_features= yogurt_long[['yogurt1','yogurt2','yogurt3','featured','price']]\nyogurt_long\n\n\n\n\n\n\n\n\nid\nproduct\ny\nfeatured\nprice\nyogurt1\nyogurt2\nyogurt3\n\n\n\n\n0\n1\n1\n0\n0\n0.108\n1\n0\n0\n\n\n1\n2\n1\n0\n0\n0.108\n1\n0\n0\n\n\n2\n3\n1\n0\n0\n0.108\n1\n0\n0\n\n\n3\n4\n1\n0\n0\n0.108\n1\n0\n0\n\n\n4\n5\n1\n0\n0\n0.125\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9715\n2426\n4\n1\n0\n0.086\n0\n0\n0\n\n\n9716\n2427\n4\n1\n0\n0.086\n0\n0\n0\n\n\n9717\n2428\n4\n1\n0\n0.086\n0\n0\n0\n\n\n9718\n2429\n4\n1\n0\n0.086\n0\n0\n0\n\n\n9719\n2430\n4\n1\n0\n0.079\n0\n0\n0\n\n\n\n\n9720 rows × 8 columns\n\n\n\n\nEstimation\n\nyogurt_labels = np.reshape(yogurt[['y1','y2','y3','y4']],(-1, ))\nyogurt_labels.shape \n\nyogurt_id = yogurt['id'].repeat(4).reset_index(drop = True)\n\nThe log-likelihood function\n\ndef multi_ll(beta, y, x, ids):\n    \n    x= x.to_numpy()\n    prob = np.exp(x.dot(beta))\n    df = pd.DataFrame({'prob':prob,'ids':ids})\n    #sum = df.groupby('ids')['prob'].sum().repeat(4).reset_index(drop= True)\n    sum_prob = df.groupby('ids')['prob'].transform('sum')\n    probs = prob/sum_prob\n    return -np.log(probs).sum()\n\n\nbeta = np.ones(5)*0\ny= yogurt_labels\nx= yogurt_features\ni= yogurt_id\nx= x.to_numpy()\nprob = np.exp(x.dot(beta))\ndf = pd.DataFrame({'prob':prob,'ids':i})\nsum = df.groupby('ids')['prob'].transform('sum')\nprobs = prob/sum \n-np.log(probs).sum()\n\n13474.781190085338\n\n\n\nyogurt_features.shape\nyogurt_labels.shape\n\n(9720,)\n\n\nFinding the MLEs for the 5 parameters (\\(\\beta_1, \\beta_2, \\beta_3, \\beta_f, \\beta_p\\)).\n\nbeta_initial = np.ones(5) * 0.1  \nresult = sp.optimize.minimize(multi_ll, beta_initial, (yogurt_labels, yogurt_features, yogurt_id), method='L-BFGS-B', options={'gtol': 1e-5, 'maxiter': 50})\n\n\n\nDiscussion\nLooking at the intercepts (0.09970444, 0.10002663 and -0.0056015) we can infer that yogurt 2 was the most preferred and yogurt 3 is the least preferred.\nTaking the diffnce between the beta of the most preferred and the least preferred ygurt we can estimate the price difference consumers are willing to pay for their preferred brand. As long as the most preferred yogurt was 12.75 cents/oz more expensive than the least preferred yogurt, consumers are willing to make that extra spend to go after thier preferred brand.\n\nintercepts = result.x[:3]  \nmost_preferred = np.argmax(intercepts)\nleast_preferred = np.argmin(intercepts)\n\nprint(\"Intercepts for Yogurt 1, 2, and 3:\", intercepts)\nprint(\"Most preferred yogurt is:\", most_preferred + 1)\nprint(\"Least preferred yogurt is:\", least_preferred + 1)\n\nIntercepts for Yogurt 1, 2, and 3: [ 0.09970444  0.10002663 -0.0056015 ]\nMost preferred yogurt is: 2\nLeast preferred yogurt is: 3\n\n\n\n#(0.10002663 +0.0056015) /0.00828118\n\nprice_coefficient = result.x[-1] \ndollar_benefit = (intercepts[most_preferred] - intercepts[least_preferred]) * -price_coefficient\n\nprint(\"Dollar benefit between the most and least preferred yogurt:\", dollar_benefit)\n\nDollar benefit between the most and least preferred yogurt: -0.0008747252446890821\n\n\nOne benefit of the MNL model is that we can simulate counterfactuals (eg, what if the price of yogurt 1 was $0.10/oz instead of $0.08/oz).\n\n\nMarket Share Analysis of Yogurt Brands\nIn our analysis of consumer prefrences for yogurt brands, we utilized the Multinomial Logit (MNL) model to estimate the current market shares and simulate the impact of a price change on consumer choices. Following results were obtained: The estimated market shares indicated that 100% of the market share was attibuted to Yogurt 1: - Yogurt 1: 100% - Yogurt 2: 0% - Yogurt 3: 0% - Yogurt 4: 0% Impact of Price Increase on Yogurt 1 To understand the sensitivity of consumer choices to price changes, we simulated a scenario where the price of Yogurt 1 was increased by $0.10 per unit. The new market shares after this price increase remained unchanged:\nYogurt 1: 100% Yogurt 2: 0% Yogurt 3: 0% Yogurt 4: 0% Key Insights Uniform Preference: The current results indicate a uniform preference for Yogurt 1 among all consumers in the dataset. This outcome suggests that either all consumers chose Yogurt 1 or there is a potential issue with the dataset or model. Based on the intercept calculation however, yogurt 2 was the most preferred choice of consumer which further underscores an issue either in the data set or the analysis model.\nPrice Insensitivity: Despite increasing the price of Yogurt 1 by $0.10, the market share for Yogurt 1 did not decrease. This implies that, within the context of the data and model, consumers are not sensitive to this price change.\nFurther Investigation Needed: The results are surprising and may indicate an underlying issue with the data or model. It is important to verify the dataset to ensure it accurately represents a diverse set of consumer preferences and to reassess the MNL model implementation to confirm it is correctly specified.\nConclusion Our initial and revised analyses using the Multinomial Logit model both yielded unexpected results, with 100% of market share attributed to Yogurt 1 regardless of a price increase. This highlights the importance of data and model validation in ensuring accurate and meaningful insights. By addressing these potential issues, we can better understand consumer preferences and market dynamics, ultimately guiding more effective pricing and promotional strategies.\n\ndef calculate_market_shares(beta, features):\n    utilities = np.dot(features, beta)\n    exp_utilities = np.exp(utilities)\n       \n    if exp_utilities.ndim == 1:\n        exp_utilities = exp_utilities.reshape(-1, 1)\n    \n    shares = exp_utilities / np.sum(exp_utilities, axis=1, keepdims=True)\n    return shares.mean(axis=0)\n\n# Current market shares\ncurrent_shares = calculate_market_shares(result.x, yogurt_features)\nprint(\"Current market shares:\", current_shares)\n\n# Simulate price increase for yogurt1\nyogurt_features_adjusted = yogurt_features.copy()\nyogurt_features_adjusted.loc[yogurt_features_adjusted['yogurt1'] == 1, 'price'] += 0.10\n\n# New market shares after price increase\nnew_shares = calculate_market_shares(result.x, yogurt_features_adjusted)\nprint(\"New market shares after price increase to yogurt 1:\", new_shares)\n\n# Check if the market shares for yogurt 1 decrease\nprint(\"Do the yogurt 1 market shares decrease?\", \"Yes\" if new_shares[0] &lt; current_shares[0] else \"No\")\n\nCurrent market shares: [1.]\nNew market shares after price increase to yogurt 1: [1.]\nDo the yogurt 1 market shares decrease? No\n\n\n\nprint(\"Shape of beta:\", beta.shape)\nprint(\"Shape of yogurt_features:\", yogurt_features.shape)\n\nShape of beta: (5,)\nShape of yogurt_features: (9720, 5)"
  },
  {
    "objectID": "projects/Project 1/index.html",
    "href": "projects/Project 1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/Project 1/index.html#introduction",
    "href": "projects/Project 1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "projects/Project 1/index.html#data",
    "href": "projects/Project 1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_stata('karlan_list_2007.dta')\ndata.describe()\n# print(np.sum(data['treatment']==1))\n# print(np.sum(data['control']==0))\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\ndata.isna().sum()\n\ntreatment                0\ncontrol                  0\nratio                    0\nratio2                   0\nratio3                   0\nsize                     0\nsize25                   0\nsize50                   0\nsize100                  0\nsizeno                   0\nask                      0\naskd1                    0\naskd2                    0\naskd3                    0\nask1                     0\nask2                     0\nask3                     0\namount                   0\ngave                     0\namountchange             0\nhpa                      0\nltmedmra                 0\nfreq                     0\nyears                    1\nyear5                    0\nmrm2                     1\ndormant                  0\nfemale                1111\ncouple                1148\nstate50one               0\nnonlit                 452\ncases                  452\nstatecnt                 0\nstateresponse            0\nstateresponset           0\nstateresponsec           3\nstateresponsetminc       3\nperbush                 35\nclose25                 35\nred0                    35\nblue0                   35\nredcty                 105\nbluecty                105\npwhite                1866\npblack                2036\npage18_39             1866\nave_hh_sz             1862\nmedian_hhincome       1874\npowner                1869\npsch_atlstba          1868\npop_propurban         1866\ndtype: int64\n\n\n\ndata= data.dropna()\ndata.shape\n\n(46513, 51)\n\n\n\ndata.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.000000\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.000000\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.000000\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.000000\n\n\n5\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\n0.862053\n0.071572\n0.363239\n2.92\n61779.0\n0.941339\n0.200840\n0.962345\n\n\n\n\n5 rows × 51 columns\n\n\n\n\n\nBalance Test\n\nm1= data[data['treatment']==1]['statecnt'].mean()\nm2= data[data['treatment']==0]['statecnt'].mean()\nn1= data[data['treatment']==1]['statecnt'].count()\nn2= data[data['treatment']==0]['statecnt'].count()\ns1= data[data['treatment']==1]['statecnt'].std()\ns2= data[data['treatment']==0]['statecnt'].std()\nse= np.sqrt((s1**2)/n1+(s2**2)/n2)\nt= (m1-m2)/se\nimport scipy.stats as stats\nstats.ttest_ind(a=data[data['treatment']==1]['statecnt'],b=data[data['treatment']==0]['statecnt'],equal_var=False)\n\nTtestResult(statistic=0.14112920381433885, pvalue=0.8877687862603023, df=31135.121177484336)\n\n\n\nprint(f'difference in means: {m1-m2:.3f}')\nprint(f't-statistic: {t:.3f}')\nprint(f'p-value: {2*(1-stats.t.cdf(np.abs(t),n1+n2)):.3f}')\n\ndifference in means: 0.008\nt-statistic: 0.141\np-value: 0.888\n\n\n\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as smf\nX = smf.add_constant(data['treatment'])\n\nx= np.array(data['treatment']).reshape(-1,1)\ny= data['statecnt']\nmodel = smf.OLS(y,X)\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               statecnt   R-squared:                       0.000\nModel:                            OLS   Adj. R-squared:                 -0.000\nMethod:                 Least Squares   F-statistic:                   0.01943\nDate:                Tue, 21 May 2024   Prob (F-statistic):              0.889\nTime:                        19:40:50   Log-Likelihood:            -1.4713e+05\nNo. Observations:               46513   AIC:                         2.943e+05\nDf Residuals:                   46511   BIC:                         2.943e+05\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          6.0518      0.046    131.652      0.000       5.962       6.142\ntreatment      0.0079      0.056      0.141      0.888      -0.102       0.118\n==============================================================================\nOmnibus:                     6798.669   Durbin-Watson:                   1.991\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            10272.870\nSkew:                           1.148   Prob(JB):                         0.00\nKurtosis:                       2.840   Cond. No.                         3.23\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n# resids = np.array(y-model.predict(x))\n# model2= LinearRegression()\n# model2.fit(resids.reshape(-1,1),x.flatten())\n# model2.coef_\n\n\nm1= data[data['treatment']==1]['mrm2'].mean()\nm2= data[data['treatment']==0]['mrm2'].mean()\nn1= data[data['treatment']==1]['mrm2'].count()\nn2= data[data['treatment']==0]['mrm2'].count()\ns1= data[data['treatment']==1]['mrm2'].std()\ns2= data[data['treatment']==0]['mrm2'].std()\nse= np.sqrt((s1**2)/n1+(s2**2)/n2)\nt= (m1-m2)/se\nimport scipy.stats as stats\nstats.ttest_ind(a=data[data['treatment']==1]['mrm2'],b=data[data['treatment']==0]['mrm2'],equal_var=False)\n\nTtestResult(statistic=0.062190468381332714, pvalue=0.9504115537887046, df=31005.961866390502)\n\n\n\nprint(f'difference in means: {m1-m2:.3f}')\nprint(f't-statistic: {t:.3f}')\nprint(f'p-value: {2*(1-stats.t.cdf(np.abs(t),n1+n2)):.3f}')\n\ndifference in means: 0.007\nt-statistic: 0.062\np-value: 0.950\n\n\n\nfrom sklearn.linear_model import LinearRegression\nx= np.array(data['treatment']).reshape(-1,1)\ny= data['mrm2']\nmodel = LinearRegression()\nmodel.fit(x,y)\nprint(f'regression coefficient: {model.coef_[0]:.3f}')\n\nregression coefficient: 0.007\n\n\n\nresids = np.array(y-model.predict(x))\nmodel2= LinearRegression()\nmodel2.fit(resids.reshape(-1,1),x.flatten())\nmodel2.coef_\n\narray([2.39341001e-18])\n\n\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another."
  },
  {
    "objectID": "projects/Project 1/index.html#experimental-results",
    "href": "projects/Project 1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nimport seaborn as sns \nsns.barplot(data = data, x= 'treatment', y = 'gave', estimator= 'mean')\n\n\n\n\n\n\n\n\n\np1= data[data['treatment']==1]['gave'].mean()\np2= data[data['treatment']==0]['gave'].mean()\nn1= data[data['treatment']==1]['gave'].count()\nn2= data[data['treatment']==0]['gave'].count()\nse= np.sqrt((p1*(1-p1)/n1+(p2*(1-p2))/n2))\nt= (p1-p2)/se\nimport scipy.stats as stats\nstats.ttest_ind(a=data[data['treatment']==1]['gave'],b=data[data['treatment']==0]['gave'],equal_var=False)\n\nTtestResult(statistic=3.5915139686322664, pvalue=0.00032921982139318785, df=34449.91883046205)\n\n\n\nprint(f'difference in proportions: {p1-p2:.3f}')\nprint(f't-statistic: {t:.3f}')\nprint(f'p-value: {2*(1-stats.t.cdf(np.abs(t),n1+n2)):.3f}')\n\ndifference in proportions: 0.005\nt-statistic: 3.592\np-value: 0.000\n\n\n\nimport statsmodels.api as smf\nx = smf.add_constant(data['treatment'])\nmodel3 = smf.Probit(data['gave'], x)\nresult = model3.fit()\nresult.summary()\n\nOptimization terminated successfully.\n         Current function value: 0.101214\n         Iterations 7\n\n\n\nProbit Regression Results\n\n\nDep. Variable:\ngave\nNo. Observations:\n46513\n\n\nModel:\nProbit\nDf Residuals:\n46511\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nTue, 21 May 2024\nPseudo R-squ.:\n0.001300\n\n\nTime:\n19:40:52\nLog-Likelihood:\n-4707.8\n\n\nconverged:\nTrue\nLL-Null:\n-4713.9\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n0.0004642\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-2.1056\n0.024\n-86.598\n0.000\n-2.153\n-2.058\n\n\ntreatment\n0.1004\n0.029\n3.466\n0.001\n0.044\n0.157\n\n\n\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\np1= data[data['ratio']==1]['gave'].mean()\np2= data[data['ratio2']==1]['gave'].mean()\nn1= data[data['ratio']==1]['gave'].count()\nn2= data[data['ratio2']==1]['gave'].count()\nse= np.sqrt((p1*(1-p1)/n1+(p2*(1-p2))/n2))\nt= (p1-p2)/se\n\nprint(f'difference in proportions: {p1-p2:.3f}')\nprint(f't-statistic: {t:.3f}')\nprint(f'p-value: {2*(1-stats.t.cdf(np.abs(t),n1+n2)):.3f}')\n\ndifference in proportions: -0.002\nt-statistic: -1.069\np-value: 0.285\n\n\n\np1= data[data['ratio']==1]['gave'].mean()\np2= data[data['ratio3']==1]['gave'].mean()\nn1= data[data['ratio']==1]['gave'].count()\nn2= data[data['ratio3']==1]['gave'].count()\nse= np.sqrt((p1*(1-p1)/n1+(p2*(1-p2))/n2))\nt= (p1-p2)/se\n\nprint(f'difference in proportions: {p1-p2:.3f}')\nprint(f't-statistic: {t:.3f}')\nprint(f'p-value: {2*(1-stats.t.cdf(np.abs(t),n1+n2)):.3f}')\n\ndifference in proportions: -0.003\nt-statistic: -1.326\np-value: 0.185\n\n\n\np1= data[data['ratio2']==1]['gave'].mean()\np2= data[data['ratio3']==1]['gave'].mean()\nn1= data[data['ratio2']==1]['gave'].count()\nn2= data[data['ratio3']==1]['gave'].count()\nse= np.sqrt((p1*(1-p1)/n1+(p2*(1-p2))/n2))\nt= (p1-p2)/se\n\nprint(f'difference in proportions: {p1-p2:.3f}')\nprint(f't-statistic: {t:.3f}')\nprint(f'p-value: {2*(1-stats.t.cdf(np.abs(t),n1+n2)):.3f}')\n\ndifference in proportions: -0.001\nt-statistic: -0.254\np-value: 0.799\n\n\nThe difference is very small and statistically insignificant\n\n#data['ratio1']= data['ratio'].astype(float)-data['ratio2']-data['ratio3']\n#data2= data[data['treatment']==1]\ndata['ratio1']= np.where((data['ratio']!='Control') & (data['ratio2']==0) & (data['ratio3']==0),1,0)\nX = smf.add_constant(data[['ratio1','ratio2','ratio3']])\nY = data['gave']\nmodel4 = smf.OLS(Y,X)\nresults = model4.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ngave\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n4.671\n\n\nDate:\nTue, 21 May 2024\nProb (F-statistic):\n0.00289\n\n\nTime:\n19:40:53\nLog-Likelihood:\n24505.\n\n\nNo. Observations:\n46513\nAIC:\n-4.900e+04\n\n\nDf Residuals:\n46509\nBIC:\n-4.897e+04\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n0.0176\n0.001\n15.349\n0.000\n0.015\n0.020\n\n\nratio1\n0.0032\n0.002\n1.778\n0.075\n-0.000\n0.007\n\n\nratio2\n0.0054\n0.002\n2.974\n0.003\n0.002\n0.009\n\n\nratio3\n0.0059\n0.002\n3.273\n0.001\n0.002\n0.009\n\n\n\n\n\n\nOmnibus:\n55346.049\nDurbin-Watson:\n2.001\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n3922911.266\n\n\nSkew:\n6.703\nProb(JB):\n0.00\n\n\nKurtosis:\n45.947\nCond. No.\n4.26\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nCompared to the treaemt and the control\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\nprint(data[data['ratio2']==1]['gave'].mean()-data[data['ratio1']==1]['gave'].mean())\nprint(data[data['ratio3']==1]['gave'].mean()-data[data['ratio2']==1]['gave'].mean())\nprint(results.params['ratio2']-results.params['ratio1'])\nprint(results.params['ratio3']-results.params['ratio2'])\n\n0.0021795099373574135\n0.000533513218346264\n0.0021795099373574053\n0.0005335132183462692\n\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\n\nX=smf.add_constant(data['treatment'])\ny= data['amount']\nmodel = smf.OLS(y,X)\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n4.480\n\n\nDate:\nTue, 21 May 2024\nProb (F-statistic):\n0.0343\n\n\nTime:\n19:40:53\nLog-Likelihood:\n-1.6706e+05\n\n\nNo. Observations:\n46513\nAIC:\n3.341e+05\n\n\nDf Residuals:\n46511\nBIC:\n3.341e+05\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n0.8048\n0.071\n11.407\n0.000\n0.667\n0.943\n\n\ntreatment\n0.1829\n0.086\n2.117\n0.034\n0.014\n0.352\n\n\n\n\n\n\nOmnibus:\n90204.552\nDurbin-Watson:\n2.006\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n230307278.792\n\n\nSkew:\n15.379\nProb(JB):\n0.00\n\n\nKurtosis:\n346.350\nCond. No.\n3.23\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nwe learn treatmetn increasess doantion aount\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\n\ndata3 = data[data['gave']==1]\nX=smf.add_constant(data3['treatment'])\ny= data3['amount']\nmodel = smf.OLS(y,X)\nresults = model.fit()\nresults.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.001\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.3300\n\n\nDate:\nTue, 21 May 2024\nProb (F-statistic):\n0.566\n\n\nTime:\n19:40:53\nLog-Likelihood:\n-5002.1\n\n\nNo. Observations:\n970\nAIC:\n1.001e+04\n\n\nDf Residuals:\n968\nBIC:\n1.002e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n45.6813\n2.545\n17.948\n0.000\n40.687\n50.676\n\n\ntreatment\n-1.7248\n3.003\n-0.574\n0.566\n-7.617\n4.167\n\n\n\n\n\n\nOmnibus:\n565.494\nDurbin-Watson:\n2.035\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n5683.123\n\n\nSkew:\n2.522\nProb(JB):\n0.00\n\n\nKurtosis:\n13.732\nCond. No.\n3.54\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nIncreasse the prob but if you were to give, the amoutn would be lower\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\n\nimport matplotlib.pyplot as plt \ntreat = data3[data3['treatment']==1]\ncontrol = data3[data3['treatment']==0]\nsns.histplot(treat, x = 'amount')\nplt.axvline(x = treat['amount'].mean(), color = 'r')\n\n\n\n\n\n\n\n\n\nsns.histplot(control, x = 'amount')\nplt.axvline(x = control['amount'].mean(), color = 'r')"
  },
  {
    "objectID": "projects/Project 1/index.html#simulation-experiment",
    "href": "projects/Project 1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\ncontrol = stats.bernoulli.rvs(0.018, size = 10000)\ntreat = stats.bernoulli.rvs(0.022, size = 10000)\ndiff = treat - control \ncumm_mean = np.cumsum(diff)/np.arange(1, 10001)\nsns.lineplot(cumm_mean, color = 'r')\nplt.axhline(y = 0.004, linestyle = '--' )\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”\n\navg50 = []\nfor i in range(1000):\n    control = stats.bernoulli.rvs(0.018, size = 50)\n    treat = stats.bernoulli.rvs(0.022, size = 50)\n    diff = treat - control \n    avg50.append(np.mean(diff))\n\ncumm_mean = np.cumsum(avg50)/np.arange(1,1001)\nsns.lineplot(cumm_mean)\nplt.axhline(y = 0.004, linestyle = '--' )\nplt.hist(x = avg50, orientation = 'horizontal')\n\n(array([  3.,  26.,  83., 192., 292., 245., 116.,  29.,   9.,   5.]),\n array([-0.1  , -0.078, -0.056, -0.034, -0.012,  0.01 ,  0.032,  0.054,\n         0.076,  0.098,  0.12 ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\navg200 = []\nfor i in range(1000):\n    control = stats.bernoulli.rvs(0.018, size = 200)\n    treat = stats.bernoulli.rvs(0.022, size = 200)\n    diff = treat - control \n    avg200.append(np.mean(diff))\n\ncumm_mean = np.cumsum(avg200)/np.arange(1,1001)\nsns.lineplot(cumm_mean)\nplt.axhline(y = 0.004, linestyle = '--' )\nplt.hist(x = avg200, orientation = 'horizontal')\n\n(array([ 13.,  95., 196., 137., 281., 175.,  57.,  34.,   9.,   3.]),\n array([-0.03  , -0.0215, -0.013 , -0.0045,  0.004 ,  0.0125,  0.021 ,\n         0.0295,  0.038 ,  0.0465,  0.055 ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\navg500 = []\nfor i in range(1000):\n    control = stats.bernoulli.rvs(0.018, size = 500)\n    treat = stats.bernoulli.rvs(0.022, size = 500)\n    diff = treat - control \n    avg500.append(np.mean(diff))\n\ncumm_mean = np.cumsum(avg500)/np.arange(1,1001)\nsns.lineplot(cumm_mean)\nplt.axhline(y = 0.004, linestyle = '--' )\nplt.hist(x = avg500, orientation = 'horizontal')\n\n(array([  7.,  23.,  88., 171., 246., 254., 141.,  48.,  19.,   3.]),\n array([-0.024 , -0.0182, -0.0124, -0.0066, -0.0008,  0.005 ,  0.0108,\n         0.0166,  0.0224,  0.0282,  0.034 ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\navg1000 = []\nfor i in range(1000):\n    control = stats.bernoulli.rvs(0.018, size = 1000)\n    treat = stats.bernoulli.rvs(0.022, size = 1000)\n    diff = treat - control \n    avg1000.append(np.mean(diff))\n\ncumm_mean = np.cumsum(avg500)/np.arange(1,1001)\nsns.lineplot(cumm_mean)\nplt.axhline(y = 0.004, linestyle = '--' )\nplt.hist(x = avg1000, orientation = 'horizontal')\n\n(array([ 15.,  35.,  61., 166., 178., 186., 203.,  93.,  38.,  25.]),\n array([-0.013 , -0.0097, -0.0064, -0.0031,  0.0002,  0.0035,  0.0068,\n         0.0101,  0.0134,  0.0167,  0.02  ]),\n &lt;BarContainer object of 10 artists&gt;)"
  }
]