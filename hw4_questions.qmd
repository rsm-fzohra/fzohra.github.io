---
title: "Key Drivers Analysis"
author: "Fatma Zohra"
date: today
---



Customer satisfaction is crucial for the success of any business, especially in the competitive world of credit cards. By understanding and analyzing what drives customer satisfaction, companies can improve their offerings, retain customers, and increase profitability. This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.



```{python}
#| echo: false
#| output: false
import pandas as pd
import numpy as np
import pyrsm as rsm 
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
from scipy.stats import pearsonr
import shap
from sklearn.inspection import permutation_importance
from xgboost import XGBRegressor

# Load the data 
data = pd.read_csv('/home/jovyan/Desktop/MGTA495-2/projects/Project 4/data_for_drivers_analysis.csv')

data.head()


```


```{python}
#| echo: false
# Define the independent variables (perceptions) and the dependent variable (satisfaction)
X = data[['trust', 'build', 'differs', 'easy', 'appealing', 'rewarding', 'popular', 'service', 'impact']]
y = data['satisfaction']

# Standardize the independent variables
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Fit a linear regression model
reg = LinearRegression().fit(X_scaled, y)
```

Standardized Regression Coefficients:
These coefficients, derived from a linear regression model, show how much change in the outcome can be expected for a one-standard-deviation change in the predictor. Higher coefficients indicate greater importance.

```{python}
#| output: false
# Standardized regression coefficients
standardized_coefficients = reg.coef_
```
Permutation Importance:
This method involves randomly permuting the values of each feature and measuring the impact on the model's performance. Features that cause a significant drop in performance when permuted are considered important. This approach approximates Shapley values, distributing the total contribution of each feature to the model's prediction accuracy.

```{python}
#| output: false
# Calculate permutation importance (as an approximation for Shapley values)
perm_importance = permutation_importance(reg, X_scaled, y, n_repeats=30, random_state=42)
perm_importance_mean = perm_importance.importances_mean
```
Random Forest Importance:
In a Random Forest model, the importance of a feature is measured by the mean decrease in the Gini impurity or the accuracy when the feature is used in splitting the data. Features that result in more homogeneous nodes when used for splitting are deemed more important.
```{python}
#| output: false
# Fit a Random Forest model for Mean Decrease in Gini Coefficient
rf = RandomForestRegressor()
rf.fit(X_scaled, y)
gini_importance = rf.feature_importances_
```
XGBoost Importance:
XGBoost provides several metrics for feature importance, such as gain (the improvement in accuracy brought by a feature to the branches it is on). This method helps identify which features are most useful in predicting the outcome.
```{python}
#| output: false
# Fit an XGBoost model and get feature importance
xgb_model = XGBRegressor()
xgb_model.fit(X_scaled, y)
xgb_importance = xgb_model.feature_importances_
```

Johnson's Relative Weights:
This technique decomposes the RÂ² of a regression model to show the relative contribution of each predictor. It provides a clear picture of which variables are driving the model's explanatory power.


```{python}
#| output: false
# Calculate Johnson's Relative Weights (using a suitable approximation)
johnson_weights = np.abs(standardized_coefficients) / np.sum(np.abs(standardized_coefficients))
```
Pearson Correlations:
Correlation coefficients measure the linear relationship between each predictor and the outcome. Higher absolute values indicate stronger associations.
```{python}
#| output: false
# Calculate Pearson Correlations
pearson_correlations = {col: pearsonr(X[col], y)[0] for col in X.columns}
```
Usefulness:
A combined measure was defined as the average of standardized coefficients, Shapley values, and Johnson's epsilon, providing a comprehensive metric of variable importance.
```{python}
#| output: false
# Calculate "Usefulness" as an average of standardized coefficients, Shapley values, and Johnson's epsilon
usefulness = (np.abs(standardized_coefficients) + perm_importance_mean + johnson_weights) / 3
```

```{python}
#| echo: false
# Create the table
results = pd.DataFrame({
    'Perception': [
        'Is offered by a brand I trust', 'Helps build credit quickly', 'Is different from other cards',
        'Is easy to use', 'Has appealing benefits or rewards', 'Rewards me for responsible usage',
        'Is used by a lot of people', 'Provides outstanding customer service', 'Makes a difference in my life'
    ],
    'Pearson Correlations (%)': [round(pearson_correlations[col] * 100, 1) for col in X.columns],
    'Standardized Multiple Regression Coefficients (%)': [round(coef * 100, 1) for coef in standardized_coefficients],
    'Shapley Values (%)': [round(value * 100, 1) for value in perm_importance_mean],
    'Johnson\'s Epsilon (%)': [round(weight * 100, 1) for weight in johnson_weights],
    'Mean Decrease in RF Gini Coefficient (%)': [round(value * 100, 1) for value in gini_importance],
    'XGBoost Importance (%)': [round(value * 100, 1) for value in xgb_importance],
    'Usefulness (%)': [round(value * 100, 1) for value in usefulness]
})

# Display the final results
results = results[['Perception', 'Pearson Correlations (%)', 'Standardized Multiple Regression Coefficients (%)',
                   'Shapley Values (%)', 'Johnson\'s Epsilon (%)', 'Mean Decrease in RF Gini Coefficient (%)',
                   'XGBoost Importance (%)', 'Usefulness (%)']]
results.index = results.index + 1

styled_df = results.style.background_gradient(cmap='Greens', axis=None, vmin=0, vmax=29)
def format_func(val):
    if isinstance(val, (int, float)):
        return f"{val:.1f}"
    return val

styled_df = styled_df.format(format_func)


styled_df
```


Variable importance refers to the process of determining which factors, among a set of predictors, have the most influence on a particular outcome. In the context of this analysis, the outcome of interest is customer satisfaction, and the predictors are various perceptions about the credit card.

This analysis provides a detailed view of how different perceptions impact overall credit card satisfaction. By using a combination of statistical methods, the most significant factors contributing to satisfaction were identified. Trust in the brand, ease of use, and quality of customer service were among the top drivers. By focusing on these key areas, credit card companies can enhance customer satisfaction, improve retention rates, and increase profitability. This comprehensive understanding of variable importance helps in making informed decisions and prioritizing efforts to meet customer needs effectively.