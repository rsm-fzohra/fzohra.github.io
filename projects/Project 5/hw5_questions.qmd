---
title: "Segmentation Methods"
author: "Fatma Zohra"
date: today
---


## K-Means

In this analysis, I explored the application of K-means clustering on the Iris dataset using both a custom algorithm and the built-in K-Means function from scikit-learn. The objective was to partition the dataset into three clusters and compare the performance of both methods. 

I started by implementing a custom K-means algorithm and then evaluated its results against the built-in function using key metrics like Within-Cluster Sum of Squares (WCSS) and silhouette score. Additionally, I extended the analysis to determine the optimal number of clusters by calculating these metrics for various values of K (2 to 7). Through this process, I aimed to understand the effectiveness and accuracy of each approach.



```{python}
#| echo: false
#| output: false
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

data = pd.read_csv('/home/jovyan/Desktop/MGTA495-2/projects/Project 5/iris.csv')
data.head()
```
## Code for Algorithm 
```{python}

#| output: false

def initialize_centroids(X, k):
    return X[np.random.choice(X.shape[0], k, replace=False)]

def assign_clusters(X, centroids):
    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
    return np.argmin(distances, axis=1)

def update_centroids(X, labels, k):
    return np.array([X[labels == i].mean(axis=0) for i in range(k)])

def plot_clusters(X, labels, centroids, iteration):
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6)
    plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X')
    plt.title(f'Iteration {iteration}')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.show()

def k_means(X, k, max_iters=100, plot_steps=False):
    centroids = initialize_centroids(X, k)
    for iteration in range(max_iters):
        labels = assign_clusters(X, centroids)
        new_centroids = update_centroids(X, labels, k)
        if plot_steps:
            plot_clusters(X, labels, centroids, iteration + 1)
        if np.all(centroids == new_centroids):
            break
        centroids = new_centroids
    return centroids, labels

def calculate_wcss(X, labels, centroids):
    wcss = 0
    for i in range(len(centroids)):
        wcss += np.sum((X[labels == i] - centroids[i]) ** 2)
    return wcss
```
## Test Algorithm on Synthetic Data 
```{python}
#| echo: false

# Generate some synthetic data for testing
np.random.seed(42)
X = np.random.rand(100, 2)  # 100 points in 2D

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Number of clusters
k = 3

# Apply K-means with plotting enabled
centroids, labels = k_means(X_scaled, k, plot_steps=True)

# Calculate WCSS
wcss = calculate_wcss(X_scaled, labels, centroids)
print(f'WCSS: {wcss}')

# Final plot
plot_clusters(X_scaled, labels, centroids, 'Final')
```

Now the algorithm is applied on the Iris data-set to calculate within-cluster-sum-of-squares.
```{python}
#| echo: false

# Apply algorithm on the Iris data set 
# #Select only numeric columns 
X_data = data.select_dtypes(include=[np.number]).values

# Standardize the features
scaler = StandardScaler()
X_data_scaled = scaler.fit_transform(X_data)

# Define the K-means functions 
def initialize_centroids(X, k):
    return X[np.random.choice(X.shape[0], k, replace=False)]

def assign_clusters(X, centroids):
    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
    return np.argmin(distances, axis=1)

def update_centroids(X, labels, k):
    return np.array([X[labels == i].mean(axis=0) for i in range(k)])

def plot_clusters(X, labels, centroids, iteration):
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6)
    plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X')
    plt.title(f'Iteration {iteration}')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.show()

def k_means(X, k, max_iters=100, plot_steps=False):
    centroids = initialize_centroids(X, k)
    for iteration in range(max_iters):
        labels = assign_clusters(X, centroids)
        new_centroids = update_centroids(X, labels, k)
        if plot_steps:
            plot_clusters(X, labels, centroids, iteration + 1)
        if np.all(centroids == new_centroids):
            break
        centroids = new_centroids
    return centroids, labels

def calculate_wcss(X, labels, centroids):
    wcss = 0
    for i in range(len(centroids)):
        wcss += np.sum((X[labels == i] - centroids[i]) ** 2)
    return wcss

# Number of clusters
k = 3  

centroids_data, labels_data = k_means(X_data_scaled, k, plot_steps=True)

# Calculate WCSS
wcss_data = calculate_wcss(X_data_scaled, labels_data, centroids_data)
print(f'WCSS: {wcss_data}')

# Final plot
plot_clusters(X_data_scaled, labels_data, centroids_data, 'Final')
```

Compare the results with the built-in _kmeans_ function of Python.
```{python}
#| echo: false

 # Preprocess the data by removing the Species column and standardizing the features
X = data.iloc[:, :-1].values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# # Define the K-means functions (as defined earlier)
def initialize_centroids(X, k):
    return X[np.random.choice(X.shape[0], k, replace=False)]

def assign_clusters(X, centroids):
    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
    return np.argmin(distances, axis=1)

def update_centroids(X, labels, k):
    return np.array([X[labels == i].mean(axis=0) for i in range(k)])

def plot_clusters(X, labels, centroids, iteration):
    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6)
    plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X')
    plt.title(f'Iteration {iteration}')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.show()

def k_means(X, k, max_iters=100, plot_steps=False):
    centroids = initialize_centroids(X, k)
    for iteration in range(max_iters):
        labels = assign_clusters(X, centroids)
        new_centroids = update_centroids(X, labels, k)
        if plot_steps:
            plot_clusters(X, labels, centroids, iteration + 1)
        if np.all(centroids == new_centroids):
            break
        centroids = new_centroids
    return centroids, labels

def calculate_wcss(X, labels, centroids):
    wcss = 0
    for i in range(len(centroids)):
        wcss += np.sum((X[labels == i] - centroids[i]) ** 2)
    return wcss

# Number of clusters
k = 3

# Apply custom K-means to the Iris dataset with plotting enabled
centroids_custom, labels_custom = k_means(X_scaled, k, plot_steps=False)

# Calculate WCSS for custom K-means
wcss_custom = calculate_wcss(X_scaled, labels_custom, centroids_custom)
print(f'Custom K-means WCSS: {wcss_custom}')

# Final plot for custom K-means
plot_clusters(X_scaled, labels_custom, centroids_custom, 'Final')

# Apply built-in KMeans
kmeans = KMeans(n_clusters=3, random_state=42)
labels_builtin = kmeans.fit_predict(X_scaled)
centroids_builtin = kmeans.cluster_centers_

# Calculate WCSS for built-in KMeans
wcss_builtin = kmeans.inertia_
print(f'Built-in KMeans WCSS: {wcss_builtin}')

# Final plot for built-in KMeans
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_builtin, cmap='viridis', alpha=0.6)
plt.scatter(centroids_builtin[:, 0], centroids_builtin[:, 1], s=300, c='red', marker='X')
plt.title('Built-in KMeans Clustering on Iris Dataset')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()
```

The built-in KMeans algorithm slightly outperforms the custom K-means implementation in terms of the WCSS, which indicates that the clusters produced by the built-in function are more compact.

Next, I determine the optimal number of clusters using both the within-cluster sum-of-squares (WCSS) and silhouette scores, using the following steps: 

1-Calculate the WCSS and silhouette scores for various numbers of clusters (K = 2, 3, ..., 7).

2-Plot the results to visualize which number of clusters is optimal based on these metrics. 
```{python}
#| echo: false

# Function to calculate WCSS and Silhouette scores for various numbers of clusters
def evaluate_clusters(X, k_range):
    wcss = []
    silhouette_scores = []
    
    for k in k_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X)
        wcss.append(kmeans.inertia_)
        silhouette_scores.append(silhouette_score(X, labels))
    
    return wcss, silhouette_scores

# Define the range of clusters
k_range = range(2, 8)

# Calculate WCSS and Silhouette scores
wcss, silhouette_scores = evaluate_clusters(X_scaled, k_range)

# Plot WCSS and Silhouette scores
plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(k_range, wcss, marker='o')
plt.title('WCSS vs. Number of Clusters')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('WCSS')

plt.subplot(1, 2, 2)
plt.plot(k_range, silhouette_scores, marker='o')
plt.title('Silhouette Score vs. Number of Clusters')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Silhouette Score')

plt.tight_layout()
plt.show()

# Print the optimal number of clusters suggested by both metrics
optimal_wcss_k = k_range[np.argmin(wcss)]
optimal_silhouette_k = k_range[np.argmax(silhouette_scores)]

print(f'Optimal number of clusters according to WCSS: {optimal_wcss_k}')
print(f'Optimal number of clusters according to Silhouette Score: {optimal_silhouette_k}')

```


In this analysis, I implemented a custom K-means clustering algorithm and compared its performance with the built-in K-Means function from scikit-learn on the Iris dataset. Both methods aimed to partition the dataset into three clusters. The performance of each approach was evaluated using two metrics: Within-Cluster Sum of Squares (WCSS) and silhouette score.

My custom K-means algorithm achieved a WCSS of 140.90 and a silhouette score of 0.459. In comparison, the built-in K-Means function produced a slightly better WCSS of 139.82 and a higher silhouette score of 0.553. These metrics indicate that the built-in algorithm formed more compact clusters and provided better separation between clusters.

The results highlight the importance of evaluating clustering algorithms using multiple metrics. WCSS helps in understanding the compactness of clusters, while silhouette scores provide insights into the separation between clusters. By comparing these metrics for different values of K, we can determine the optimal number of clusters and ensure effective clustering performance. This analysis underscores the value of comprehensive evaluation in selecting and fine-tuning clustering algorithms.






